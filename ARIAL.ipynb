{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "005b3a38-1ca6-4105-bf9a-04e859aaa02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\koust\\anaconda3\\lib\\site-packages (8.3.205)\n",
      "Requirement already satisfied: timm in c:\\users\\koust\\anaconda3\\lib\\site-packages (1.0.20)\n",
      "Requirement already satisfied: torchvision in c:\\users\\koust\\anaconda3\\lib\\site-packages (0.23.0+cu129)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\koust\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\koust\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from ultralytics) (2.1.3)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from ultralytics) (3.10.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from ultralytics) (1.15.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from ultralytics) (2.8.0+cu129)\n",
      "Requirement already satisfied: psutil in c:\\users\\koust\\anaconda3\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: polars in c:\\users\\koust\\anaconda3\\lib\\site-packages (from ultralytics) (1.34.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from ultralytics) (2.0.17)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\koust\\anaconda3\\lib\\site-packages (from timm) (0.35.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\koust\\anaconda3\\lib\\site-packages (from timm) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\koust\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\koust\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\koust\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\koust\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (72.1.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\koust\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Requirement already satisfied: polars-runtime-32==1.34.0 in c:\\users\\koust\\anaconda3\\lib\\site-packages (from polars->ultralytics) (1.34.0)\n"
     ]
    }
   ],
   "source": [
    "# Setup: install necessary libraries (run in a Python environment)\n",
    "!pip install ultralytics timm torchvision scikit-learn tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c2c9dec-99af-4f99-9be9-0edd41545e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ultralytics import YOLO  # YOLOv8 API\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e53cdd76-8ddc-4b88-a54c-9aaa92144607",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisDroneDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.transform = transform\n",
    "        # List all image files\n",
    "        self.image_files = [f for f in os.listdir(images_dir) if f.endswith('.jpg')]\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.images_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.labels_dir, self.image_files[idx].replace('.jpg','.txt'))\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        # Read YOLO labels\n",
    "        boxes, labels = [], []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path) as f:\n",
    "                for line in f:\n",
    "                    cls, x, y, w, h = map(float, line.split())\n",
    "                    labels.append(int(cls))\n",
    "                    # Convert normalized centers to pixel coords (optional)\n",
    "                    boxes.append([x, y, w, h])\n",
    "        target = {\"boxes\": torch.tensor(boxes), \"labels\": torch.tensor(labels)}\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, target\n",
    "\n",
    "# Example usage (paths to be set appropriately)\n",
    "# train_ds = VisDroneDataset('VisDrone/images/train', 'VisDrone/labels/train', transform=transforms.ToTensor())\n",
    "# train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af41c059-3e4b-454b-9b9c-d908d3cde9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained YOLOv8n model\n",
    "model = YOLO(\"yolov8n.pt\")  # COCO-pretrained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f04d682-b4ae-40b2-8558-d64f36d53cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\koust\\OneDrive\\Pictures\\maa.jpg: 640x512 1 person, 27.7ms\n",
      "Speed: 1.9ms preprocess, 27.7ms inference, 75.1ms postprocess per image at shape (1, 3, 640, 512)\n"
     ]
    }
   ],
   "source": [
    "#Dummy example: run on one image\n",
    "image = r\"C:\\Users\\koust\\OneDrive\\Pictures\\maa.jpg\"\n",
    "results = model(image)  # returns detection results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "babd5e91-07eb-40ff-8bc8-d5b3ef6cff8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.205  Python-3.13.5 torch-2.8.0+cu129 CUDA:0 (NVIDIA GeForce RTX 5070, 12226MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 1419.5357.0 MB/s, size: 120.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\koust\\datasets\\VisDrone\\labels\\val.cache... 548 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 548/548 1.3Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 35/35 11.7it/s 3.0s0.1s\n",
      "                   all        548      38759     0.0639     0.0486     0.0306      0.014\n",
      "                person        520       8844      0.274      0.179      0.143      0.061\n",
      "               bicycle        482       5125     0.0722   0.000976     0.0471     0.0152\n",
      "                   car        364       1287    0.00221      0.028    0.00146   0.000472\n",
      "            motorcycle        515      14064    0.00762   0.000356     0.0305     0.0196\n",
      "              airplane        421       1975     0.0666   0.000506    0.00569    0.00372\n",
      "                   bus        266        750     0.0698      0.145      0.031     0.0199\n",
      "                 train        337       1045          0          0    0.00246    0.00177\n",
      "                 truck        220        532     0.0184      0.115    0.00888    0.00465\n",
      "                  boat        131        251    0.00533    0.00398    0.00396    0.00289\n",
      "         traffic light        485       4886      0.123     0.0133     0.0325     0.0105\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\koust\\runs\\detect\\val2\u001b[0m\n",
      "mAP50-95: 0.013967855702685345 mAP50: 0.03064233737548234\n"
     ]
    }
   ],
   "source": [
    "# Validate YOLOv8n on VisDrone (requires dataset YAML and GPU)\n",
    "metrics = model.val(data=\"VisDrone.yaml\", imgsz=640)\n",
    "print(\"mAP50-95:\", metrics.box.map, \"mAP50:\", metrics.box.map50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c3cc709-def3-443e-b94e-52e3670fbb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "# Load pretrained classifiers\n",
    "mobilenet = timm.create_model('mobilenetv3_small_100', pretrained=True)\n",
    "vit = timm.create_model('vit_tiny_patch16_224', pretrained=True)\n",
    "mobilenet.eval()\n",
    "vit.eval()\n",
    "\n",
    "# Example transform for classifier input (ImageNet preprocess)\n",
    "clf_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e3a6a70-f4dd-46fa-b5a2-f830decf4d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80, F1-score: 0.78\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "y_true = [0, 1, 2, 1, 0]   # example ground truth classes\n",
    "y_pred = [0, 2, 2, 1, 0]   # example model predictions\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "print(f\"Accuracy: {acc:.2f}, F1-score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec8e080b-d1ed-4b42-8437-00b5654b37c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_detections(image, boxes, labels):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    for (x, y, w, h), cls in zip(boxes, labels):\n",
    "        # Convert normalized xywh to pixel rectangle\n",
    "        imgW, imgH = image.size\n",
    "        x1 = (x - w/2) * imgW; y1 = (y - h/2) * imgH\n",
    "        x2 = (x + w/2) * imgW; y2 = (y + h/2) * imgH\n",
    "        rect = plt.Rectangle((x1,y1), x2-x1, y2-y1,\n",
    "                             edgecolor='red', facecolor='none', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1-5, str(cls), color='red', fontsize=12)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb466af-522f-46fd-94c3-7958885d1746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time for batch of 4: 0.037 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mobilenet.to(device)\n",
    "\n",
    "# Simulate timing on a dummy batch of images\n",
    "dummy = torch.randn(4, 3, 224, 224).to(device)  # batch of 4\n",
    "start = time.time()\n",
    "_ = mobilenet(dummy)\n",
    "torch.cuda.synchronize() if device.type=='cuda' else None\n",
    "print(f\"Inference time for batch of 4: {time.time()-start:.3f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e250ef00-7177-419a-a63c-a7ad3ff6479f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Starting inference. Press 'q' in the window to quit.\n",
      "End of stream or cannot fetch frame.\n",
      "Processed 3794 frames in 60.38s  Avg FPS: 62.83\n"
     ]
    }
   ],
   "source": [
    "# Real-time YOLOv8 detection from webcam or video (single Jupyter cell)\n",
    "# Paste and run in a notebook cell.\n",
    "\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "# -------- User config --------\n",
    "VIDEO_SOURCE = r\"C:\\Users\\koust\\Downloads\\dhaka_traffic.mp4\\dhaka_traffic.mp4\"                 # 0 = default webcam, or 'path/to/video.mp4'\n",
    "MODEL_WEIGHTS = \"yolov8n.pt\"     # pretrained COCO weights (ultralytics)\n",
    "IMG_SZ = 640                     # inference size (reasonable tradeoff)\n",
    "CONF_THRESH = 0.35               # detection confidence threshold\n",
    "SHOW_LABELS = True               # draw class name + conf\n",
    "write_output = False             # set True to save annotated output to file\n",
    "output_path = \"out_detected.mp4\" # only used if write_output=True\n",
    "# ------------------------------\n",
    "\n",
    "# Device check\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Load YOLOv8n model (pretrained). This will download weights if not present.\n",
    "model = YOLO(MODEL_WEIGHTS)\n",
    "# Make sure model runs on chosen device\n",
    "model.to(device)\n",
    "\n",
    "# Open video capture\n",
    "cap = cv2.VideoCapture(VIDEO_SOURCE)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Could not open video source: {VIDEO_SOURCE}\")\n",
    "\n",
    "# Prepare writer if saving output\n",
    "writer = None\n",
    "if write_output:\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) if cap.get(cv2.CAP_PROP_FPS) > 0 else 25.0\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    print(\"Saving annotated output to:\", output_path)\n",
    "\n",
    "# Colors for drawing\n",
    "np.random.seed(42)\n",
    "palette = (np.random.randint(0,255, size=(80,3))).tolist()  # up to 80 COCO classes\n",
    "\n",
    "# Helper to draw boxes on frame\n",
    "def draw_boxes(frame, boxes, scores, classes, class_names):\n",
    "    for (x1,y1,x2,y2), s, c in zip(boxes, scores, classes):\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "        color = tuple(int(x) for x in palette[int(c) % len(palette)])\n",
    "        cv2.rectangle(frame, (x1,y1), (x2,y2), color, 2)\n",
    "        if SHOW_LABELS:\n",
    "            label = f\"{class_names[int(c)]} {s:.2f}\"\n",
    "            (w,h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "            cv2.rectangle(frame, (x1, y1 - 18), (x1 + w, y1), color, -1)\n",
    "            cv2.putText(frame, label, (x1, y1 - 3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
    "\n",
    "# Get class names from model (if available)\n",
    "try:\n",
    "    class_names = model.model.names\n",
    "except Exception:\n",
    "    # fallback COCO names\n",
    "    class_names = {i: str(i) for i in range(80)}\n",
    "\n",
    "# Main loop: read frames, run inference, draw, display\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "avg_fps = 0.0\n",
    "# use warm-up frame(s) to prepare model, reduce variance\n",
    "warmup_frames = 2\n",
    "\n",
    "print(\"Starting inference. Press 'q' in the window to quit.\")\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of stream or cannot fetch frame.\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    # Convert BGR->RGB for model (ultralytics expects np.ndarray RGB or path)\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Inference: pass frame directly; set device via model\n",
    "    t0 = time.time()\n",
    "    # note: model() returns a Results object list; we use first element\n",
    "    results = model.predict(source=[img_rgb], imgsz=IMG_SZ, device=device, conf=CONF_THRESH, verbose=False)\n",
    "    # results is a list corresponding to the batch (here single image)\n",
    "    res = results[0]\n",
    "    # extract bounding boxes, confidences, classes\n",
    "    # res.boxes.xyxy is tensor (N,4), res.boxes.conf (N,), res.boxes.cls (N,)\n",
    "    if hasattr(res, \"boxes\") and len(res.boxes) > 0:\n",
    "        boxes = res.boxes.xyxy.cpu().numpy()\n",
    "        scores = res.boxes.conf.cpu().numpy()\n",
    "        classes = res.boxes.cls.cpu().numpy()\n",
    "    else:\n",
    "        boxes = np.array([])\n",
    "        scores = np.array([])\n",
    "        classes = np.array([])\n",
    "\n",
    "    # Draw detections on the original BGR frame\n",
    "    if boxes.size:\n",
    "        draw_boxes(frame, boxes, scores, classes, class_names)\n",
    "\n",
    "    # Calculate FPS (use torch.cuda.synchronize for accurate GPU timing)\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    fps = 1.0 / (t1 - t0) if (t1 - t0) > 0 else 0.0\n",
    "    # running avg fps\n",
    "    avg_fps = (avg_fps * (frame_count - 1) + fps) / frame_count\n",
    "\n",
    "    # Overlay FPS and stats on frame\n",
    "    info_text = f\"FPS: {fps:.1f}  AvgFPS: {avg_fps:.1f}  Detections: {len(boxes)}\"\n",
    "    cv2.putText(frame, info_text, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLOv8 Real-time\", frame)\n",
    "\n",
    "    # Write out if requested\n",
    "    if write_output and writer is not None:\n",
    "        writer.write(frame)\n",
    "\n",
    "    # Exit on 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "if writer is not None:\n",
    "    writer.release()\n",
    "cv2.destroyAllWindows()\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Processed {frame_count} frames in {total_time:.2f}s  Avg FPS: {frame_count/total_time:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
